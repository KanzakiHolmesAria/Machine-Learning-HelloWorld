{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预备知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装&导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install pytorch torchvision torchaudio -c pytorch\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(4, 3)\n",
    "x = torch.randn(4, 3) # randn正态分布、rand均匀分布\n",
    "x = torch.zeros(4, 3, dtype=torch.long) # 64位整数\n",
    "\n",
    "x = torch.tensor([5.5, 3]) # 常数向量\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8127, -0.8060]], dtype=torch.float64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.XX_like: 生成和x相同size的新向量\n",
    "y = torch.randn_like(x, dtype=torch.float64)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运算操作符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9874, 1.3534],\n",
       "        [1.1401, 1.7519],\n",
       "        [1.4237, 1.3908]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量运算\n",
    "x = torch.ones(3, 2)\n",
    "y = torch.rand(3, 2)\n",
    "\n",
    "# x + y\n",
    "# torch.add(x, y)\n",
    "# x.add(y)\n",
    "res = torch.empty_like(x)\n",
    "torch.add(x, y, out=res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin x: tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "inplace x: tensor([[1.9874, 1.3534],\n",
      "        [1.1401, 1.7519],\n",
      "        [1.4237, 1.3908]])\n"
     ]
    }
   ],
   "source": [
    "# pytorch操作inplace版本均有后缀\"_\" \n",
    "x = torch.ones(3, 2)\n",
    "print('origin x:', x)\n",
    "x.add_(y)\n",
    "print('inplace x:', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改变形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n",
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# y、z与x同源，一个变化其余跟着变化\n",
    "x = torch.ones(3, 4)\n",
    "print(x)\n",
    "y = x.view(2, 6)\n",
    "z = x.reshape(2, 6)\n",
    "y += 1\n",
    "print(x)\n",
    "print(z)\n",
    "\n",
    "y = x.clone() # 取消同源\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor和Numpy转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1837])\n",
      "-0.18372656404972076\n"
     ]
    }
   ],
   "source": [
    "# 将标量tensor转化为python number\n",
    "x = torch.randn(1)\n",
    "x_np = x.item()\n",
    "print(x)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.]) [1. 1. 1.]\n",
      "tensor([2., 2., 2.]) [2. 2. 2.]\n",
      "[1. 1. 1.] tensor([1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2.] tensor([2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Tensor to Numpy(a, b共享内存)\n",
    "a = torch.ones(3)\n",
    "b = a.numpy()\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "\n",
    "# Numpy to Tensor(a, b共享内存)\n",
    "a = np.ones(3)\n",
    "b = torch.from_numpy(a)\n",
    "print(a, b)\n",
    "\n",
    "a += 1\n",
    "print(a, b)\n",
    "\n",
    "# 不共享内存的方法：\n",
    "a = np.ones(3)\n",
    "b = torch.tensor(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  自动求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None <AddBackward0 object at 0x7fc1fdf79e10>\n",
      "True False\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1.0000, 0.1000],\n",
      "        [0.0100, 0.0010]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(x.grad_fn, y.grad_fn)\n",
    "print(x.is_leaf, y.is_leaf)\n",
    "print(out)\n",
    "\n",
    "# 反向传播，计算梯度\n",
    "out.backward() # output维度是1，所以backward里默认是Tensor(1)\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "y.backward(torch.ones_like(x)) # y维度不是1，所以需要使用同行向量适配维度\n",
    "print(x.grad) # 每次backward会叠加，所以需要清空\n",
    "\n",
    "x.grad.data.zero_()\n",
    "y.backward(torch.ones_like(x))\n",
    "print(x.grad)\n",
    "\n",
    "x.grad.data.zero_()\n",
    "v = torch.Tensor([[1, 0.1], [0.01, 0.001]])\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习基础 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from torch import nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建网络的四种方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net1(\n",
      "  (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dense1): Linear(in_features=288, out_features=128, bias=True)\n",
      "  (dense2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "Net2(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (0): Linear(in_features=288, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Net3(\n",
      "  (conv): Sequential(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu1): ReLU()\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (dense1): Linear(in_features=288, out_features=128, bias=True)\n",
      "    (relu2): ReLU()\n",
      "    (dense2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Net4(\n",
      "  (conv): Sequential(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Relu1): ReLU()\n",
      "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dense): Sequential(\n",
      "    (dense1): Linear(in_features=288, out_features=128, bias=True)\n",
      "    (Relu2): ReLU()\n",
      "    (dense2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "方式一：直接搭建\n",
    "'''\n",
    "class Net1(nn.Module):\n",
    "    # 初始化，在init里搭建网络\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 32, 3, 1, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dense1 = nn.Linear(32 * 3 * 3, 128)\n",
    "        self.dense2 = nn.Linear(128, 10)\n",
    "\n",
    "    # 假设x为input变量，前向计算出需要返回的结果output\n",
    "    def forward(self, x):\n",
    "        a1 = self.pool(self.relu(self.conv(x))).reshape(x.size(0), -1)\n",
    "        a2 = self.dense2(self.relu(self.dense1(a1)))\n",
    "        return a2\n",
    "\n",
    "net1 = Net1()\n",
    "print(net1)\n",
    "\n",
    "'''\n",
    "方式二：利用Sequential类\n",
    "'''\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, 3, 1, 1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.dense = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32 * 3 * 3, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        res = conv_out.reshape(conv_out.size(0), -1)\n",
    "        out = self.dense(res)\n",
    "        return out\n",
    "\n",
    "net2 = Net2()\n",
    "print(net2)\n",
    "\n",
    "'''\n",
    "方式三：利用add_module方法\n",
    "'''\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net3, self).__init__()\n",
    "        self.conv = torch.nn.Sequential()\n",
    "        self.conv.add_module(\"conv1\", torch.nn.Conv2d(3, 32, 3, 1, 1))\n",
    "        self.conv.add_module(\"relu1\", torch.nn.ReLU())\n",
    "        self.conv.add_module(\"pool1\", torch.nn.MaxPool2d(2))\n",
    "        self.dense = torch.nn.Sequential()\n",
    "        self.dense.add_module('dense1', torch.nn.Linear(32 * 3 * 3, 128))\n",
    "        self.dense.add_module('relu2', torch.nn.ReLU())\n",
    "        self.dense.add_module('dense2', torch.nn.Linear(128, 10))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        res = conv_out.reshape(conv_out.size(0), -1)\n",
    "        out = self. dense(res)\n",
    "        return out\n",
    "\n",
    "net3 = Net3()\n",
    "print(net3)\n",
    "\n",
    "\n",
    "'''\n",
    "方式三：利用有序字典传入模块\n",
    "'''\n",
    "class Net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net4, self).__init__()\n",
    "        self.conv  = torch.nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"conv1\", torch.nn.Conv2d(3, 32, 3, 1, 1)),\n",
    "                    (\"Relu1\", torch.nn.ReLU()),\n",
    "                    (\"pool\", torch.nn.MaxPool2d(2))\n",
    "                ]))\n",
    "        self.dense  = torch.nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"dense1\", torch.nn.Linear(32 * 3 * 3, 128)),\n",
    "                    (\"Relu2\", torch.nn.ReLU()),\n",
    "                    (\"dense2\", torch.nn.Linear(128, 10))\n",
    "                ])) \n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        res = conv_out.reshape(conv_out.size(0), -1)\n",
    "        out = self. dense(res)\n",
    "        return out\n",
    "\n",
    "net4 = Net4()\n",
    "print(net4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
